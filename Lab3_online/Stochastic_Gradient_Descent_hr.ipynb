{"cells":[{"cell_type":"markdown","metadata":{"id":"nLLyQT9Erw7f"},"source":["# Uvod #\n","\n","U prve dvije lekcije naučili smo kako izgraditi potpuno povezane mreže od gomile gustih slojeva. Prilikom prve izrade, sve težine mreže postavljene su nasumično -- mreža još ništa ne \"zna\". U ovoj lekciji ćemo vidjeti kako trenirati neuronsku mrežu; vidjet ćemo kako neuronske mreže *uče*.\n","\n","Kao i kod svih zadataka strojnog učenja, počinjemo sa skupom podataka za obuku. Svaki primjer u podacima o obuci sastoji se od nekih značajki (ulaza) zajedno s očekivanim ciljem (izlaz). Uvježbavanje mreže znači podešavanje njezinih težina na takav način da može transformirati značajke u cilj. U skupu podataka *80 žitarica*, na primjer, želimo mrežu koja može uzeti sadržaj `'sugars'`, `'fibers'` i `'proteins'` svake žitarice i proizvesti predviđanje `'calories'` te žitarice. Ako možemo uspješno uvježbati mrežu da to čini, njezine težine moraju na neki način predstavljati odnos između tih značajki i tog cilja kako je izraženo u podacima o uvježbavanju.\n","\n","Osim podataka za treniranje, potrebne su nam još dvije stvari:\n","- \"Funkcija gubitka\" koja mjeri koliko su dobra predviđanja mreže.\n","- \"Optimizator\" koji može reći mreži kako promijeniti svoje težine.\n","\n","# Funkcija gubitka #\n","\n","Vidjeli smo kako dizajnirati arhitekturu za mrežu, ali nismo vidjeli kako mreži reći *koji* problem treba riješiti. Ovo je posao funkcije gubitka.\n","\n","**Funkcija gubitka** mjeri disparitet između prave vrijednosti cilja i vrijednosti koju model predviđa.\n","\n","Različiti problemi zahtijevaju različite funkcije gubitaka. Gledali smo **regresijske** probleme, gdje je zadatak predvidjeti neku brojčanu vrijednost -- kalorije u *80 žitarica*, ocjena u *Red wine quality*. Drugi zadaci regresije mogu biti predviđanje cijene kuće ili učinkovitosti goriva automobila.\n","\n","Uobičajena funkcija gubitka za regresijske probleme je **srednja apsolutna pogreška** ili **MAE**. Za svako predviđanje \"y_pred\", MAE mjeri odstupanje od pravog cilja \"y_true\" apsolutnom razlikom \"abs(y_true - y_pred)\".\n","\n","Ukupni MAE gubitak na skupu podataka srednja je vrijednost svih ovih apsolutnih razlika.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/VDcvkZN.png\" width=\"500\" alt=\"A graph depicting error bars from data points to the fitted line..\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Srednja apsolutna pogreška prosječna je duljina između prilagođene krivulje i podatkovnih točaka.\n","</center></figcaption>\n","</figure>\n","\n","Osim MAE, druge funkcije gubitka koje možete vidjeti za regresijske probleme su srednje kvadratna pogreška (MSE) ili Huberov gubitak (obje dostupne u Kerasu).\n","\n","Tijekom treninga, model će koristiti funkciju gubitka kao vodič za pronalaženje točnih vrijednosti svojih težina (manji gubitak je bolji). Drugim riječima, funkcija gubitka govori mreži njezin cilj.\n","\n","# Optimizator - Stohastički gradijentni pad #\n","\n","Opisali smo problem koji želimo da mreža riješi, ali sada moramo reći *kako* ga riješiti. Ovo je posao **optimizatora**. Optimizator je algoritam koji prilagođava težine kako bi se smanjio gubitak.\n","\n","Gotovo svi algoritmi optimizacije koji se koriste u dubokom učenju pripadaju obitelji koja se naziva **stohastičko gradijentno spuštanje - stohastic gradient descend - SGD**. To su iterativni algoritmi koji treniraju mrežu u koracima. Jedan **korak** treninga ide ovako:\n","1. Uzmite uzorke podataka o treniranju i provucite ih kroz mrežu ih da napravite predviđanja.\n","2. Izmjerite gubitak između predviđanja i pravih vrijednosti.\n","3. Na kraju, namjestite težine u smjeru koji smanjuje gubitak.\n","\n","Zatim samo radite ovo iznova i iznova dok gubitak ne bude onoliko mali koliko želite (ili dok se više ne smanji).\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"figs/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Treniranje neuronske mreže sa Stohastičkim gradijentnim spuštanjem - SGD.\n","</center></figcaption>\n","</figure>\n","\n","Uzorak podataka za obuku svake iteracije naziva se **minibatch** (ili često samo \"serija\"), dok se kompletan krug podataka za obuku naziva **epoha**. Broj epoha za koje trenirate je koliko će puta mreža vidjeti svaki primjer prilikom treniranja.\n","\n","Animacija prikazuje linearni model koji se trenira sa SGD. Blijedocrvene točke prikazuju cijeli set za treniranje, dok su pune crvene točke miniserije. Svaki put kada SGD vidi novu miniseriju, pomaknut će težine (`w` nagib i `b` y-odsječak) prema njihovim točnim vrijednostima na toj seriji. Serija za serijom, linija na kraju konvergira kako bi najbolje pristajala. Možete vidjeti da gubitak postaje manji kako se težine približavaju svojim pravim vrijednostima.\n","\n","## Stopa učenja i veličina serije ##\n","\n","Primijetite da se linija samo malo pomiče u smjeru svake serije (umjesto da se pomakne do kraja). Veličina tih pomaka određena je **stopom učenja**. Manja stopa učenja znači da mreža mora vidjeti više miniserija prije nego što njezine težine konvergiraju svojim najboljim vrijednostima.\n","\n","Stopa učenja i veličina miniserija dva su parametra koji imaju najveći učinak prilikom SGD traniranja. Njihova je interakcija često suptilna i pravi izbor za te parametre nije uvijek očit. (Istražit ćemo ove učinke u vježbi.)\n","\n","Srećom, za većinu poslova neće biti potrebno napraviti opsežno pretraživanje hiperparametara da bi se dobili zadovoljavajući rezultati. **Adam** je SGD algoritam koji ima prilagodljivu stopu učenja koja ga čini prikladnim za većinu problema bez ikakvog podešavanja parametara (to je \"samopodešavanje\", u određenom smislu). Adam je izvrstan optimizator opće namjene.\n","\n","## Dodavanje Gubitka i Optimizatora ##\n","\n","Nakon definiranja modela, možete dodati funkciju gubitka i optimizator metodom `compile` modela:\n","\n","```\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=\"mae\",\n",")\n","```\n","\n","Primijetite da možemo specificirati gubitak i optimizator. Također im možete pristupiti izravno putem Keras API-ja -- ako želite podesiti parametre, na primjer -- ali za nas će zadane vrijednosti funkcionirati dobro.\n","\n","<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n","<strong>Što je u imenu?</strong><br>\n","<strong>Gradijent</strong> je vektor koji nam govori u kojem smjeru trebaju ići težine. Točnije, govori nam kako promijeniti težine da bi se gubitak promijenio <em>najbrže</em>. Gradijent našeg procesa nazivamo <strong>spuštanje</strong> jer koristi gradijent za <em>spuštanje</em> krivulje gubitaka prema minimumu. <strong>Stohastički</strong> znači \"određen slučajno.\" Naše treniranje je <em>stohastičko</em> jer su miniserije <em>nasumični uzorci</em> iz skupa podataka. I zato se zove SGD!\n","</blockquote>"]},{"cell_type":"markdown","metadata":{"id":"3HL_PFgurw7g"},"source":["# Primjer - Kvaliteta crnog vina #\n","\n","Sada znamo sve što nam je potrebno za početak obuke modela dubokog učenja. Pa da vidimo na djelu! Koristit ćemo skup podataka *Red wine quality*.\n","\n","Ovaj skup podataka sastoji se od fizikalno-kemijskih mjerenja za oko 1600 portugalskih crvenih vina. Također je uključena ocjena kvalitete za svako vino iz slijepih proba. Koliko dobro možemo predvidjeti percipiranu kvalitetu vina iz ovih mjerenja?\n","\n","Stavili smo sve pripreme podataka u ovu sljedeću skrivenu ćeliju. Nije bitno za ono što slijedi pa ga slobodno preskočite. Jedna stvar koju biste za sada mogli primijetiti jest da smo svaku značajku ponovno skalirali tako da leži u intervalu $[0, 1]$. Kao što ćemo više raspravljati kasnije, neuronske mreže obično imaju najbolje rezultate kada su njihovi ulazi na zajedničkoj ljestvici."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"id":"JzRR_TuOrw7g","lines_to_next_cell":0},"outputs":[],"source":["\n","import pandas as pd\n","from IPython.display import display\n","\n","red_wine = pd.read_csv('./input/red-wine.csv')\n","\n","# Create training and validation splits\n","df_train = red_wine.sample(frac=0.7, random_state=0)\n","df_valid = red_wine.drop(df_train.index)\n","display(df_train.head(4))\n","\n","# Scale to [0, 1]\n","max_ = df_train.max(axis=0)\n","min_ = df_train.min(axis=0)\n","df_train = (df_train - min_) / (max_ - min_)\n","df_valid = (df_valid - min_) / (max_ - min_)\n","\n","# Split features and target\n","X_train = df_train.drop('quality', axis=1)\n","X_valid = df_valid.drop('quality', axis=1)\n","y_train = df_train['quality']\n","y_valid = df_valid['quality']"]},{"cell_type":"markdown","metadata":{"id":"8ixeOBWnrw7g"},"source":["Koliko bi ova mreža trebala imati ulaza? To možemo otkriti gledajući broj stupaca u matrici podataka - dataframeu. Pazite da ovdje ne uključite cilj (`'quality'`) -- samo ulazne značajke."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTGmHJBZrw7g","lines_to_next_cell":2},"outputs":[],"source":["print(X_train.shape)"]},{"cell_type":"markdown","metadata":{"id":"Spy38T7Wrw7g"},"source":["Jedanaest stupaca znači jedanaest ulaza.\n","\n","Odabrali smo troslojnu mrežu s preko 1500 neurona. Ova bi mreža trebala biti sposobna naučiti prilično složene odnose u podacima."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik2vguC0rw7g"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential([\n","    layers.Dense(512, activation='relu', input_shape=[11]),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(1),\n","])"]},{"cell_type":"markdown","metadata":{"id":"q0Q6LZn3rw7g"},"source":["Odlučivanje o arhitekturi vašeg modela trebalo bi biti dio procesa. Počnite jednostavno i koristite gubitak valjanosti kao svoj vodič. Na vježbama ćete naučiti više o razvoju modela.\n","\n","Nakon definiranja modela, kompiliramo u optimizaciju i funkciju gubitka."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bc64Ebjyrw7g"},"outputs":[],"source":["model.compile(\n","    optimizer='adam',\n","    loss='mae',\n",")"]},{"cell_type":"markdown","metadata":{"id":"SG8YngIXrw7g"},"source":["Sada smo spremni za početak treninga! Rekli smo Kerasu da optimizatoru unese 256 redaka podataka za obuku odjednom (`batch_size`) i da to učini 10 puta kroz cijeli skup podataka (`epochs`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOCQyFD-rw7g"},"outputs":[],"source":["history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_valid, y_valid),\n","    batch_size=256,\n","    epochs=10,\n",")"]},{"cell_type":"markdown","metadata":{"id":"gBS2cASrrw7g"},"source":["Možete vidjeti da će vas Keras obavještavati o gubitku dok model trenira.\n","\n","Često je bolji način da vidite gubitak iscrtati ga. Metoda `fit` zapravo vodi evidenciju o gubicima nastalim tijekom obuke u objektu `History`. Pretvorit ćemo podatke u Pandas dataframe, što olakšava iscrtavanje."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kd4ELghDrw7h","lines_to_next_cell":2},"outputs":[],"source":["import pandas as pd\n","\n","# convert the training history to a dataframe\n","history_df = pd.DataFrame(history.history)\n","# use Pandas native plot method\n","history_df['loss'].plot();"]},{"cell_type":"markdown","metadata":{"id":"4d-apBXUrw7h"},"source":["Primijetite kako se gubitak smanjuje kako epohe prolaze. Kada krivulja gubitaka postane tako horizontalna, to znači da je model naučio sve što je mogao i ne bi bilo razloga za nastavak dodatnih epoha."]},{"cell_type":"markdown","metadata":{"id":"ewTAkPuWrw7h"},"source":["# Tvoj red #\n","\n","Sada, [**koristi stochastic gradient descent**](Stochastic_Gradient_Descent_exercise_hr.ipynb) za treniranje tvoje mreže."]}],"metadata":{"colab":{"name":"Stochastic Gradient Descent","provenance":[]},"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
